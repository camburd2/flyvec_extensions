{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "Want context dependent and static\n",
    "see how well our dictionary lines up with the datasets - We skip pretty much all of them in MEN: 2087\n",
    "different hash codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I got RW, RG-65, and Mturk: https://github.com/vecto-ai/word-benchmarks/tree/master/word-similarity/monolingual/en.\n",
    "has other stuff too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class FlyVecGPU:\n",
    "    def __init__(self, N_size, kc_size, debug, device):\n",
    "        self.W = torch.rand(kc_size, 2 * N_size, device=device)\n",
    "        self.debug = debug\n",
    "        self.device = device\n",
    "        #print(self.W) #prints random matrix\n",
    "\n",
    "    def predict(self, i):\n",
    "        i = i.to(self.device)\n",
    "        return torch.matmul(self.W, i)\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save(self.W, path)\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        #For .pth\n",
    "        # self.W = torch.load(path, map_location=self.device) # Note the change here!\n",
    "\n",
    "        #for .pt\n",
    "        loaded_data = torch.load(path, map_location=self.device)  # Load the entire dictionary\n",
    "        self.W = loaded_data['W']\n",
    "\n",
    "        #torch.set_printoptions(profile=\"full\")\n",
    "        print(self.W)\n",
    "\n",
    "    def learning(self, context_target_pair, probability_vector, learning_rate):\n",
    "        # Ensure tensors are on the correct device\n",
    "        context_target_pair = context_target_pair.to(self.device)\n",
    "        probability_vector = probability_vector.to(self.device)\n",
    "        learning_rate = torch.tensor(learning_rate, device=self.device)\n",
    "\n",
    "        normalized_input = context_target_pair * probability_vector\n",
    "        activations = torch.matmul(self.W, normalized_input)\n",
    "        max_neuron = torch.argmax(activations)\n",
    "        if self.debug:\n",
    "            print(f\"Max neuron: {max_neuron.item()}\")\n",
    "            print(f\"Activations: {activations[max_neuron].item()}\")\n",
    "\n",
    "        max_neuron_weights = self.W[max_neuron]\n",
    "\n",
    "        # Vectorized weight update\n",
    "        update = learning_rate * (normalized_input - max_neuron_weights * normalized_input * max_neuron_weights)\n",
    "        self.W[max_neuron] += update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class StreamedTokenizer:\n",
    "    def __init__(self, vocab_dict_file, vocab_freq_file, input_file, device, window_size=5):\n",
    "        self.vocab_dict = {}\n",
    "        self.vocab_freq = {}\n",
    "        with open(vocab_dict_file, 'r') as f:\n",
    "            self.vocab_dict = json.load(f)\n",
    "        with open(vocab_freq_file, 'r') as f:\n",
    "            self.vocab_freq = json.load(f)\n",
    "        self.input_file_handle = open(input_file, 'r')\n",
    "        self.window_size = window_size\n",
    "        self.device = device\n",
    "        self.N_size = len(self.vocab_dict)\n",
    "\n",
    "        vocab_size = len(self.vocab_dict)\n",
    "        self.probability_vector = torch.zeros(vocab_size * 2, dtype=torch.float, device=self.device)\n",
    "        total_freq = sum(self.vocab_freq.values())\n",
    "        for word, freq in self.vocab_freq.items():\n",
    "            self.probability_vector[self.vocab_dict[word]] = freq / total_freq\n",
    "            self.probability_vector[vocab_size + self.vocab_dict[word]] = freq / total_freq\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        # Read a line from the input file\n",
    "        for line in self.input_file_handle:\n",
    "            if line.startswith(' = ') or line.startswith('= '):\n",
    "                continue\n",
    "\n",
    "            tokens = []\n",
    "            for word in line.split():\n",
    "                tokens.append(self.vocab_dict[word])\n",
    "\n",
    "            half_window = self.window_size // 2\n",
    "            vocab_size = len(self.vocab_dict)\n",
    "\n",
    "            for i, target_word_idx in enumerate(tokens):\n",
    "                # Get context indices\n",
    "                context_indices = tokens[max(0, i - half_window):i] + tokens[i + 1:i + 1 + half_window]\n",
    "\n",
    "                # Create context and target vectors as tensors\n",
    "                context_vector = torch.zeros(vocab_size, dtype=torch.int, device=self.device)\n",
    "                for idx in context_indices:\n",
    "                    if idx < vocab_size:\n",
    "                        context_vector[idx] = 1\n",
    "\n",
    "                target_vector = torch.zeros(vocab_size, dtype=torch.int, device=self.device)\n",
    "                if target_word_idx < vocab_size:\n",
    "                    target_vector[target_word_idx] = 1\n",
    "\n",
    "                # Concatenate context and target vectors\n",
    "                input_vector = torch.cat([context_vector, target_vector])\n",
    "\n",
    "                yield input_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import csv\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Load the vocabulary dictionary\n",
    "with open('dict.json', 'r') as f:\n",
    "    vocab_dict = json.load(f)\n",
    "\n",
    "N_size = len(vocab_dict)\n",
    "kc_size = 400  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_embedding(activation, k):\n",
    "    # Get indices of top-k activations\n",
    "    topk_indices = torch.topk(activation, k=k).indices\n",
    "    binary_embedding = torch.zeros_like(activation, dtype=torch.int)\n",
    "    binary_embedding[topk_indices] = 1\n",
    "    return binary_embedding.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_similarity(v1, v2):\n",
    "    n = len(v1)\n",
    "    n11 = torch.sum((v1 == 1) & (v2 == 1)).item()\n",
    "    n00 = torch.sum((v1 == 0) & (v2 == 0)).item()\n",
    "    sim = (n11 + n00) / n\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_path, embeddings):\n",
    "    word_pairs = []\n",
    "    human_scores = []\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        sample_line = f.readline()\n",
    "        if ',' in sample_line:\n",
    "            delimiter = ','\n",
    "        elif '\\t' in sample_line:\n",
    "            delimiter = '\\t'\n",
    "        else:\n",
    "            delimiter = ' '\n",
    "        reader = csv.reader(f, delimiter=delimiter)\n",
    "        for row in reader:\n",
    "            if len(row) < 3:\n",
    "                continue\n",
    "            word1 = row[0]\n",
    "            word2 = row[1]\n",
    "            score = float(row[2])\n",
    "            word_pairs.append((word1, word2))\n",
    "            human_scores.append(score)\n",
    "\n",
    "    model_scores = []\n",
    "    filtered_human_scores = []\n",
    "    skips = 0\n",
    "    for (word1, word2), human_score in zip(word_pairs, human_scores):\n",
    "        if word1 in embeddings and word2 in embeddings:\n",
    "            emb1 = embeddings[word1]\n",
    "            emb2 = embeddings[word2]\n",
    "            sim = binary_similarity(emb1, emb2)\n",
    "            # print(\"Word1: \", word1)\n",
    "            # print(\"Word2: \", word2)\n",
    "            # print(\"emb1: \", emb1)\n",
    "            # print(\"emb2: \", emb2)\n",
    "            # print(\"sim: \", sim)\n",
    "            # print(\"Human: \", human_score)\n",
    "            model_scores.append(sim)\n",
    "            filtered_human_scores.append(human_score)\n",
    "        else:\n",
    "            skips += 1\n",
    "            \n",
    "            continue  # Skip pairs where words are not in vocabulary\n",
    "    print(\"Skips: \", skips)\n",
    "\n",
    "    # Compute Spearman correlation\n",
    "    if len(model_scores) > 1:\n",
    "        spearman_corr, _ = spearmanr(model_scores, filtered_human_scores)\n",
    "        return spearman_corr * 100  # Convert to percentage\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datasets = {\n",
    "    'MEN': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/MEN/MEN_dataset_natural_form_full.txt',\n",
    "    'WS353': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/WordSim353/combined_processed.tab',\n",
    "    'SimLex': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/SimLex-999/SimLex-999_processed.txt',\n",
    "    'RW': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/RW/rw_processed.txt',\n",
    "    'RG': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/RG65/rg-65_processed.txt',\n",
    "    'Mturk': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/Mturk/mturk-771_processed.txt',\n",
    "}\n",
    "\n",
    "folder_path = '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/paths'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    print(\"File: \", file_path)\n",
    "\n",
    "    model = FlyVecGPU(N_size, kc_size, debug=False, device=device)\n",
    "    model.load_checkpoint(file_path)\n",
    "    #prints random matrix and learned matrix just to check\n",
    "\n",
    "    # Hash lengths to evaluate\n",
    "    hash_lengths = [51, 4, 8, 16, 32, 64, 128]\n",
    "    test = True\n",
    "    # Evaluate the model for each hash length\n",
    "    for k in hash_lengths:\n",
    "        print(f\"\\nHash Length (k): {k}\")\n",
    "        embeddings = {}\n",
    "        for word, idx in vocab_dict.items():\n",
    "            # Create input vector for the word\n",
    "            input_vector = torch.zeros(2 * N_size, dtype=torch.float, device=device)\n",
    "            input_vector[N_size + idx] = 1.0  # Set target word index\n",
    "            # Compute activations\n",
    "            activation = model.predict(input_vector)\n",
    "            # Binarize the activations using top-k\n",
    "            binary_embedding = get_binary_embedding(activation, k)\n",
    "            if test:\n",
    "                # print(\"activation:\")\n",
    "                # print(activation)\n",
    "                # print(\"embedding:\")\n",
    "                # print(binary_embedding)\n",
    "                test = False\n",
    "            embeddings[word] = binary_embedding\n",
    "\n",
    "        # Evaluate the model on each dataset\n",
    "        results = {}\n",
    "        for name, path in datasets.items():\n",
    "            score = evaluate_dataset(path, embeddings)\n",
    "            if score is not None:\n",
    "                results[name] = score\n",
    "                #print(f\"{name}: {score:.1f}\\n\")\n",
    "            else:\n",
    "                print(f\"{name}: Not enough data to compute Spearman correlation.\")\n",
    "\n",
    "        # Optionally, print results in a table format\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"Dataset\\tSpearman Correlation (%)\")\n",
    "        for dataset_name, score in results.items():\n",
    "            print(f\"{dataset_name}\\t{score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With plots for .pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the following are defined elsewhere:\n",
    "# N_size, kc_size, device, vocab_dict, FlyVecGPU, get_binary_embedding, evaluate_dataset\n",
    "\n",
    "datasets = {\n",
    "    'MEN': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/MEN/MEN_dataset_natural_form_full.txt',\n",
    "    'WS353': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/WordSim353/combined_processed.tab',\n",
    "    'SimLex': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/SimLex-999/SimLex-999_processed.txt',\n",
    "    'RW': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/RW/rw_processed.txt',\n",
    "    'RG': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/RG65/rg-65_processed.txt',\n",
    "    'Mturk': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/Mturk/mturk-771_processed.txt',\n",
    "}\n",
    "\n",
    "folder_path = '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/paths'\n",
    "\n",
    "hash_lengths = [51, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Function to extract numeric part from file names like '0.pth', '1.pth', etc.\n",
    "def extract_numeric_part(file_name):\n",
    "    base_name = os.path.splitext(os.path.basename(file_name))[0]  # Get '0' from '0.pth'\n",
    "    try:\n",
    "        return int(base_name)\n",
    "    except ValueError:\n",
    "        return None  # Non-numeric files will be skipped\n",
    "\n",
    "# Get and sort file names based on their numeric value\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.pth')]\n",
    "\n",
    "# Filter out files without numeric parts\n",
    "file_names = [f for f in file_names if extract_numeric_part(f) is not None]\n",
    "\n",
    "# Sort the file names based on their numeric value\n",
    "file_names.sort(key=lambda x: extract_numeric_part(x))\n",
    "\n",
    "for file_name in file_names:\n",
    "    iteration = extract_numeric_part(file_name)\n",
    "    if iteration is None:\n",
    "        continue  # Skip files without a numeric iteration value\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    file_label = os.path.basename(file_name)\n",
    "    print(\"File: \", file_path)\n",
    "\n",
    "    model = FlyVecGPU(N_size, kc_size, debug=False, device=device)\n",
    "    model.load_checkpoint(file_path)\n",
    "    # Prints random matrix and learned matrix just to check\n",
    "\n",
    "    test = True\n",
    "\n",
    "    for k in hash_lengths:\n",
    "        print(f\"\\nHash Length (k): {k}\")\n",
    "        embeddings = {}\n",
    "        for word, idx in vocab_dict.items():\n",
    "            # Create input vector for the word\n",
    "            input_vector = torch.zeros(2 * N_size, dtype=torch.float, device=device)\n",
    "            input_vector[N_size + idx] = 1.0  # Set target word index\n",
    "            # Compute activations\n",
    "            activation = model.predict(input_vector)\n",
    "            # Binarize the activations using top-k\n",
    "            binary_embedding = get_binary_embedding(activation, k)\n",
    "            if test:\n",
    "                test = False\n",
    "            embeddings[word] = binary_embedding\n",
    "\n",
    "        # Evaluate the model on each dataset\n",
    "        for dataset_name, dataset_path in datasets.items():\n",
    "            score = evaluate_dataset(dataset_path, embeddings)\n",
    "            if score is not None and np.isfinite(score):\n",
    "                results.append({\n",
    "                    'iteration': iteration,\n",
    "                    'file_name': file_path,\n",
    "                    'file_label': file_label,\n",
    "                    'hash_length': k,\n",
    "                    'dataset': dataset_name,\n",
    "                    'score': score\n",
    "                })\n",
    "            else:\n",
    "                print(f\"{dataset_name}: Not enough data to compute Spearman correlation or score is not finite.\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Remove any rows with non-finite values\n",
    "df = df[np.isfinite(df['iteration']) & np.isfinite(df['score'])]\n",
    "\n",
    "# Create 'plots' directory if it doesn't exist\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Plotting\n",
    "for dataset_name in df['dataset'].unique():\n",
    "    # Create subfolder for the dataset\n",
    "    dataset_folder = os.path.join('plots', dataset_name)\n",
    "    os.makedirs(dataset_folder, exist_ok=True)\n",
    "    for k in df['hash_length'].unique():\n",
    "        df_subset = df[(df['dataset'] == dataset_name) & (df['hash_length'] == k)]\n",
    "        df_subset = df_subset.sort_values('iteration')\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df_subset['iteration'], df_subset['score'], marker='o')\n",
    "        plt.title(f'Dataset: {dataset_name}, Hash Length: {k}')\n",
    "        plt.xlabel('Iteration (File Name)')\n",
    "        plt.ylabel('Spearman Correlation (%)')\n",
    "        plt.xticks(df_subset['iteration'], df_subset['file_label'], rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        # Save the figure in the dataset's folder\n",
    "        output_file = os.path.join(dataset_folder, f'{dataset_name}_k{k}.png')\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With plots for .pt FOR CAMS MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note, the vocab size needs to be even, just because i didnt feel like changing the model above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skips:  546 of 3000 for MEN\n",
    "Skips:  52 of 353 for WS353\n",
    "Skips:  92 of 1000 for SimLex\n",
    "Skips:  1718 of 2034 For RW\n",
    "Skips:  24 of 65 For RG\n",
    "Skips:  40 of 771 For MTURK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19570\n"
     ]
    }
   ],
   "source": [
    "# For open_web\n",
    "import json\n",
    "\n",
    "# Load vocab\n",
    "flyvec_embeddings_path = 'simple-flyvec-embeddings.json'\n",
    "with open(flyvec_embeddings_path, 'r') as file:\n",
    "    embeddings = json.load(file)\n",
    "\n",
    "\n",
    "vocab_dict = {word: idx for idx, word in enumerate(embeddings.keys())}\n",
    "\n",
    "N_size = len(vocab_dict)\n",
    "kc_size = 400  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(N_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Assuming the following are defined elsewhere:\n",
    "# N_size, kc_size, device, vocab_dict, FlyVecGPU, get_binary_embedding, evaluate_dataset\n",
    "\n",
    "datasets = {\n",
    "    'MEN': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/MEN/MEN_dataset_natural_form_full.txt',\n",
    "    'WS353': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/WordSim353/combined_processed.tab',\n",
    "    'SimLex': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/SimLex-999/SimLex-999_processed.txt',\n",
    "    'RW': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/RW/rw_processed.txt',\n",
    "    'RG': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/RG65/rg-65_processed.txt',\n",
    "    'Mturk': '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/Mturk/mturk-771_processed.txt',\n",
    "}\n",
    "\n",
    "folder_path = '/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/498_project/flyvec_extensions/trained_models/context_openwebtext_checkpoints'\n",
    "\n",
    "hash_lengths = [51, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Function to extract numeric part from file names like '0.pth', '1.pth', etc.\n",
    "def extract_numeric_part(file_name):\n",
    "    base_name = os.path.splitext(os.path.basename(file_name))[0]  # Remove extension\n",
    "    match = re.search(r'\\d+', base_name)  # Find the first numeric part\n",
    "    if match:\n",
    "        #print(int(match.group()), flush = True)\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        print(\"errro\", flush = True)\n",
    "\n",
    "# Get and sort file names based on their numeric value\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.pt')]\n",
    "\n",
    "# Filter out files without numeric parts\n",
    "file_names = [f for f in file_names if extract_numeric_part(f) is not None]\n",
    "\n",
    "# Sort the file names based on their numeric value\n",
    "file_names.sort(key=lambda x: extract_numeric_part(x))\n",
    "\n",
    "for file_name in file_names:\n",
    "    iteration = extract_numeric_part(file_name)\n",
    "    if iteration is None:\n",
    "        continue  # Skip files without a numeric iteration value\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    file_label = os.path.basename(file_name)\n",
    "    print(\"File: \", file_path)\n",
    "\n",
    "    model = FlyVecGPU(int(N_size/2), kc_size, debug=False, device=device)\n",
    "    model.load_checkpoint(file_path)\n",
    "    # Prints random matrix and learned matrix just to check\n",
    "\n",
    "    test = True\n",
    "\n",
    "    for k in hash_lengths:\n",
    "        print(f\"\\nHash Length (k): {k}\")\n",
    "        embeddings = {}\n",
    "        for word, idx in vocab_dict.items():\n",
    "            # Create input vector for the word\n",
    "            input_vector = torch.zeros(N_size, dtype=torch.float, device=device)\n",
    "            input_vector[idx] = 1.0  # Set target word index\n",
    "            # Compute activations\n",
    "            activation = model.predict(input_vector)\n",
    "            # Binarize the activations using top-k\n",
    "            binary_embedding = get_binary_embedding(activation, k)\n",
    "            if test:\n",
    "                test = False\n",
    "            embeddings[word] = binary_embedding\n",
    "\n",
    "        # Evaluate the model on each dataset\n",
    "        for dataset_name, dataset_path in datasets.items():\n",
    "            score = evaluate_dataset(dataset_path, embeddings)\n",
    "            if score is not None and np.isfinite(score):\n",
    "                results.append({\n",
    "                    'iteration': iteration,\n",
    "                    'file_name': file_path,\n",
    "                    'file_label': file_label,\n",
    "                    'hash_length': k,\n",
    "                    'dataset': dataset_name,\n",
    "                    'score': score\n",
    "                })\n",
    "                print(results)\n",
    "            else:\n",
    "                print(f\"{dataset_name}: Not enough data to compute Spearman correlation or score is not finite.\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(results)\n",
    "\n",
    "# Remove any rows with non-finite values\n",
    "df = df[np.isfinite(df['iteration']) & np.isfinite(df['score'])]\n",
    "\n",
    "# Create 'plots' directory if it doesn't exist\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Plotting\n",
    "for dataset_name in df['dataset'].unique():\n",
    "    # Create subfolder for the dataset\n",
    "    dataset_folder = os.path.join('plots', dataset_name)\n",
    "    os.makedirs(dataset_folder, exist_ok=True)\n",
    "    for k in df['hash_length'].unique():\n",
    "        df_subset = df[(df['dataset'] == dataset_name) & (df['hash_length'] == k)]\n",
    "        df_subset = df_subset.sort_values('iteration')\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df_subset['iteration'], df_subset['score'], marker='o')\n",
    "        plt.title(f'Dataset: {dataset_name}, Hash Length: {k}')\n",
    "        plt.xlabel('Iteration (File Name)')\n",
    "        plt.ylabel('Spearman Correlation (%)')\n",
    "        plt.xticks(df_subset['iteration'], df_subset['file_label'], rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        # Save the figure in the dataset's folder\n",
    "        output_file = os.path.join(folder_path, f'{dataset_name}_k{k}.png')\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlyVecGPU(N_size, kc_size, debug=False, device=device)\n",
    "model.load_checkpoint('/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/498_project/flyvec_extensions/trained_models/context_openwebtext_checkpoints/model_checkpoint_1pct.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlyVecGPU(N_size, kc_size, debug=False, device=device)\n",
    "model.load_checkpoint('/mnt/c/Users/kobeh/OneDrive/eecs445/Project_2/flyvec/flyvec/table_7_recreation/paths/30.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'number'\n",
    "hash_len = 32\n",
    "k = hash_len\n",
    "\n",
    "input_vector = torch.zeros(2 * N_size, dtype=torch.float, device=device)\n",
    "input_vector[N_size + vocab_dict.get(word)] = 1.0  # Set target word index\n",
    "# Compute activations\n",
    "activation = model.predict(input_vector)\n",
    "\n",
    "enc_target_word = get_binary_embedding(activation, k)\n",
    "print(enc_target_word)\n",
    "\n",
    "sims = []\n",
    "\n",
    "\n",
    "def calc_sim(v1, v2):\n",
    "\n",
    "    n = len(v1)\n",
    "    n11 = torch.sum((v1 == 1) & (v2 == 1)).item()\n",
    "    n00 = torch.sum((v1 == 0) & (v2 == 0)).item()\n",
    "    #print(n11, \" : \", n00)\n",
    "    sim = (n11 + n00) / n\n",
    "    return sim\n",
    "\n",
    "\n",
    "# Calculate  similarity between target and the rest of vocab\n",
    "for word in vocab_dict.keys():\n",
    "    input_vector = torch.zeros(2 * N_size, dtype=torch.float, device=device)\n",
    "    input_vector[N_size + vocab_dict.get(word)] = 1.0  # Set target word index\n",
    "    # Compute activations\n",
    "    activation = model.predict(input_vector)\n",
    "    enc_word = get_binary_embedding(activation, k)\n",
    "\n",
    "    sim = calc_sim(enc_target_word, enc_word)\n",
    "    sims.append((word, sim))\n",
    "\n",
    "\n",
    "# Sort by similarity score and get top N\n",
    "N = 15\n",
    "top_N = sorted(sims, key=lambda x: x[1], reverse=True)[:N]\n",
    "bottom_N = sorted(sims, key=lambda x: x[1], reverse=False)[:N]\n",
    "\n",
    "# Print results\n",
    "print(f\"{'Word':<15} {'Similarity':<10} {'Frequency':<10}\")\n",
    "print(\"-\" * 35)\n",
    "for word, sim in top_N:\n",
    "    print(f\"{word:<15} {sim:>9.3f}\")\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"{'Word':<15} {'Similarity':<10} {'Frequency':<10}\")\n",
    "print(\"-\" * 35)\n",
    "for word, sim in bottom_N:\n",
    "    print(f\"{word:<15} {sim:>9.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envp2",
   "language": "python",
   "name": "envp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
