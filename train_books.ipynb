{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\camer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from context_model import ContextModel\n",
    "import preprocess_books as prep\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (232446, 10)\n",
      "train sample: ['elements' 'regular' '<unk>' 'moral' 'material' 'world' 'depend'\n",
      " 'complained' 'smoking' 'blood']\n"
     ]
    }
   ],
   "source": [
    "# Load data and combine books into a string\n",
    "combined_books_text = prep.load_book_data('data/train.json')\n",
    "\n",
    "# Clean/filter text\n",
    "words_list, word_counts, vocab = prep.preprocess_text(combined_books_text)\n",
    "\n",
    "# Create training data: np array shape [N, window_size]\n",
    "train_data = prep.prepare_training_data(words_list, window_size=10)\n",
    "\n",
    "\n",
    "print(f'train data shape: {train_data.shape}')\n",
    "print(f'train sample: {random.choice(train_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████████████████████████████████████| 232446/232446 [01:43<00:00, 2251.27it/s]\n",
      "Epoch 2/3: 100%|██████████████████████████████████████████| 232446/232446 [01:47<00:00, 2161.46it/s]\n",
      "Epoch 3/3: 100%|██████████████████████████████████████████| 232446/232446 [01:49<00:00, 2118.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = ContextModel(\n",
    "    K_size= 350,            # Number of neurons\n",
    "    vocab_size=len(vocab),  # Size of vocab\n",
    "    k=5,                    # Update top-k neurons\n",
    "    lr=.1,                  # Learning rate\n",
    "    norm_rate=5             # Normalization rate\n",
    ")\n",
    "\n",
    "# Create encoder\n",
    "enc = utils.Encoder(vocab)\n",
    "\n",
    "# Train model\n",
    "num_epochs = 3\n",
    "\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    for num, sample in enumerate(tqdm(train_data, desc=f'Epoch {i+1}/{num_epochs}', ncols=100, leave=True)):        \n",
    "        enc_sample = enc.one_hot(sample)\n",
    "        model.update(enc_sample)\n",
    "\n",
    "# Save model\n",
    "utils.save_model(model, f'trained_models/context_model_epoch{num_epochs}_books.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Similarity Frequency \n",
      "-----------------------------------\n",
      "ship                1.000       1203\n",
      "vessel              0.811        366\n",
      "ocean               0.811        322\n",
      "land                0.806        932\n",
      "waves               0.806        288\n",
      "conseil             0.806        285\n",
      "_nautilus_          0.806        507\n",
      "shore               0.800        586\n",
      "ned                 0.800        359\n",
      "canadian            0.800        145\n",
      "seas                0.794        216\n",
      "sea                 0.789       1717\n",
      "board               0.789        575\n",
      "boat                0.789       1038\n",
      "screw               0.789         90\n",
      "crew                0.789        311\n",
      "floating            0.789        158\n",
      "observations        0.783         68\n",
      "island              0.783        628\n",
      "ice                 0.783        239\n"
     ]
    }
   ],
   "source": [
    "# Find words with embeddings most similar to the target word embedding\n",
    "target_word = 'ship'\n",
    "hash_length = 70\n",
    "top_N_closest = 20\n",
    "\n",
    "#model = utils.load_model('trained_models/context_model_epoch3_books.pt')\n",
    "\n",
    "utils.calc_print_sim_words(\n",
    "    vocab=vocab,\n",
    "    word_counts=word_counts,\n",
    "    model=model,\n",
    "    word=target_word,\n",
    "    hash_len=hash_length,\n",
    "    top_N=top_N_closest\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
